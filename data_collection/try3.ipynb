{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc16b63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000000, 10), (2000000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = \"./ninapro_db1/Ninapro_DB1.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df = df.iloc[:2000000]\n",
    "emg_cols = [c for c in df.columns if \"emg\" in c.lower()]\n",
    "stim = df[\"stimulus\"]\n",
    "stim = stim.squeeze()\n",
    "df = df[emg_cols]\n",
    "\n",
    "df.shape, stim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03b1536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "def emg_filter_pipeline(x, fs, band=(20, 90), notch=60, smooth_lp=5.0, order=4, rectify=False):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    nyq = fs * 0.5\n",
    "\n",
    "    def _filtfilt(b, a, sig):\n",
    "        return signal.filtfilt(b, a, sig, axis=0, method=\"gust\")\n",
    "\n",
    "    # Bandpass\n",
    "    low, high = band\n",
    "    high = min(high, nyq * 0.999)               # keep strictly < Nyquist\n",
    "    assert 0 < low < high < nyq, \"Invalid band edges for given fs.\"\n",
    "    b_bp, a_bp = signal.butter(order, [low/nyq, high/nyq], btype=\"band\")\n",
    "    y = _filtfilt(b_bp, a_bp, x)\n",
    "\n",
    "    # Notch 60Hz\n",
    "    if notch is not None:\n",
    "        w0 = notch / nyq\n",
    "        # Quality factor: higher Q = narrower notch (30â€“50 works well for EMG mains hum)\n",
    "        b_notch, a_notch = signal.iirnotch(w0, Q=30)\n",
    "        y = _filtfilt(b_notch, a_notch, y)\n",
    "\n",
    "    # Rectifying\n",
    "    if rectify:\n",
    "        y = np.abs(y)\n",
    "\n",
    "    # Smoothing\n",
    "    if smooth_lp is not None:\n",
    "        cutoff = min(smooth_lp, nyq * 0.999)\n",
    "        b_lp, a_lp = signal.butter(order, cutoff/nyq, btype=\"low\")\n",
    "        y = _filtfilt(b_lp, a_lp, y)\n",
    "\n",
    "    return y\n",
    "\n",
    "df = emg_filter_pipeline(\n",
    "    df, \n",
    "    fs = 100,\n",
    "    band = (5, 45),\n",
    "    notch = None,\n",
    "    smooth_lp = None,\n",
    "    order = 4,\n",
    "    rectify = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6df0c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "window_size = 200\n",
    "\n",
    "def window(df, win_size, stride = 1):\n",
    "    windows = []\n",
    "    for start in range(0, df.shape[0] - win_size, stride):\n",
    "        windows.append(df[start:start+win_size])\n",
    "    return np.stack(windows, axis=0)\n",
    "\n",
    "data = window(df, win_size=window_size, stride = 10)\n",
    "y = window(stim, win_size=window_size, stride = 10)\n",
    "y = mode(y, axis=1)[0].squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbebb433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((199980, 200, 10), (199980,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c13a348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(data).float()\n",
    "x = x.permute(0, 2, 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97b52bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_size, num_layers, n_classes):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=7, padding=3)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        # self.relu = F.relu()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.hidden_dim = 64\n",
    "        self.lstm = nn.LSTM(input_size = self.hidden_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0.3 if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_size, 64)\n",
    "        self.fc2 = nn.Linear(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        # x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        # x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        out = lstm_out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        x = F.relu(self.fc(out))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "541d317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "class EMGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        y = self.y[index]\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(x, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "mu = X_train.mean(axis=(0, 2), keepdims = True)\n",
    "sig = X_train.std(axis=(0, 2), keepdims = True) + 1e-8\n",
    "\n",
    "X_train = (X_train - mu) / sig\n",
    "X_val = (X_val - mu) / sig\n",
    "X_test = (X_test - mu) / sig\n",
    "\n",
    "\n",
    "train = EMGDataset(X_train, y_train)\n",
    "val = EMGDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12df18eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c1/t6c91th93rg_t8cmmzfjwp840000gn/T/ipykernel_98741/4236329272.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=2.1711, train_acc=0.427, val_acc=0.439\n",
      "Epoch 2: train_loss=2.1275, train_acc=0.432, val_acc=0.403\n",
      "Epoch 3: train_loss=2.2216, train_acc=0.425, val_acc=0.429\n",
      "Epoch 4: train_loss=2.2166, train_acc=0.426, val_acc=0.426\n",
      "Epoch 5: train_loss=2.1961, train_acc=0.427, val_acc=0.423\n",
      "Epoch 6: train_loss=2.1787, train_acc=0.429, val_acc=0.053\n",
      "Epoch 7: train_loss=2.1726, train_acc=0.429, val_acc=0.426\n",
      "Epoch 8: train_loss=2.1695, train_acc=0.430, val_acc=0.416\n",
      "Epoch 9: train_loss=2.1656, train_acc=0.431, val_acc=0.426\n",
      "Epoch 10: train_loss=2.1543, train_acc=0.432, val_acc=0.431\n",
      "Epoch 11: train_loss=2.1539, train_acc=0.433, val_acc=0.430\n",
      "Epoch 12: train_loss=2.1539, train_acc=0.434, val_acc=0.430\n",
      "Epoch 13: train_loss=2.1519, train_acc=0.432, val_acc=0.392\n",
      "Epoch 14: train_loss=2.1510, train_acc=0.433, val_acc=0.434\n",
      "Epoch 15: train_loss=2.1488, train_acc=0.432, val_acc=0.437\n",
      "Epoch 16: train_loss=2.1520, train_acc=0.432, val_acc=0.434\n",
      "Epoch 17: train_loss=2.1507, train_acc=0.433, val_acc=0.436\n",
      "Epoch 18: train_loss=2.1474, train_acc=0.433, val_acc=0.435\n",
      "Epoch 19: train_loss=2.1454, train_acc=0.433, val_acc=0.434\n",
      "Epoch 20: train_loss=2.1423, train_acc=0.434, val_acc=0.436\n",
      "Epoch 21: train_loss=2.1453, train_acc=0.434, val_acc=0.429\n",
      "Epoch 22: train_loss=2.1462, train_acc=0.434, val_acc=0.434\n",
      "Epoch 23: train_loss=2.1464, train_acc=0.433, val_acc=0.430\n",
      "Epoch 24: train_loss=2.1413, train_acc=0.433, val_acc=0.426\n",
      "Epoch 25: train_loss=2.1477, train_acc=0.433, val_acc=0.433\n",
      "Epoch 26: train_loss=2.1435, train_acc=0.434, val_acc=0.437\n",
      "Epoch 27: train_loss=2.1480, train_acc=0.434, val_acc=0.431\n",
      "Epoch 28: train_loss=2.1502, train_acc=0.433, val_acc=0.430\n",
      "Epoch 29: train_loss=2.1470, train_acc=0.434, val_acc=0.426\n",
      "Epoch 30: train_loss=2.1526, train_acc=0.433, val_acc=0.431\n",
      "Epoch 31: train_loss=2.1515, train_acc=0.433, val_acc=0.429\n",
      "Epoch 32: train_loss=2.1477, train_acc=0.432, val_acc=0.431\n",
      "Epoch 33: train_loss=2.1482, train_acc=0.433, val_acc=0.418\n",
      "Epoch 34: train_loss=2.1517, train_acc=0.433, val_acc=0.435\n",
      "Epoch 35: train_loss=2.1541, train_acc=0.433, val_acc=0.428\n",
      "Epoch 36: train_loss=2.1571, train_acc=0.432, val_acc=0.433\n",
      "Epoch 37: train_loss=2.1578, train_acc=0.433, val_acc=0.432\n",
      "Epoch 38: train_loss=2.1569, train_acc=0.432, val_acc=0.428\n",
      "Epoch 39: train_loss=2.1593, train_acc=0.431, val_acc=0.432\n",
      "Epoch 40: train_loss=2.1567, train_acc=0.432, val_acc=0.405\n",
      "Epoch 41: train_loss=2.1574, train_acc=0.432, val_acc=0.435\n",
      "Epoch 42: train_loss=2.1598, train_acc=0.431, val_acc=0.422\n",
      "Epoch 43: train_loss=2.1635, train_acc=0.431, val_acc=0.434\n",
      "Epoch 44: train_loss=2.1629, train_acc=0.431, val_acc=0.416\n",
      "Epoch 45: train_loss=2.1633, train_acc=0.432, val_acc=0.423\n",
      "Epoch 46: train_loss=2.1637, train_acc=0.431, val_acc=0.431\n",
      "Epoch 47: train_loss=2.1635, train_acc=0.432, val_acc=0.429\n",
      "Epoch 48: train_loss=2.1712, train_acc=0.430, val_acc=0.422\n",
      "Epoch 49: train_loss=2.1701, train_acc=0.431, val_acc=0.433\n",
      "Epoch 50: train_loss=2.1714, train_acc=0.431, val_acc=0.432\n",
      "Epoch 51: train_loss=2.1679, train_acc=0.431, val_acc=0.429\n",
      "Epoch 52: train_loss=2.1684, train_acc=0.430, val_acc=0.425\n",
      "Epoch 53: train_loss=2.1696, train_acc=0.430, val_acc=0.385\n",
      "Epoch 54: train_loss=2.1658, train_acc=0.432, val_acc=0.432\n",
      "Epoch 55: train_loss=2.1698, train_acc=0.430, val_acc=0.430\n",
      "Epoch 56: train_loss=2.1699, train_acc=0.431, val_acc=0.431\n",
      "Epoch 57: train_loss=2.1711, train_acc=0.430, val_acc=0.428\n",
      "Epoch 58: train_loss=2.1692, train_acc=0.431, val_acc=0.420\n",
      "Epoch 59: train_loss=2.1681, train_acc=0.430, val_acc=0.432\n",
      "Epoch 60: train_loss=2.1685, train_acc=0.431, val_acc=0.434\n",
      "Epoch 61: train_loss=2.1706, train_acc=0.430, val_acc=0.426\n",
      "Epoch 62: train_loss=2.1788, train_acc=0.429, val_acc=0.433\n",
      "Epoch 63: train_loss=2.1846, train_acc=0.429, val_acc=0.425\n",
      "Epoch 64: train_loss=2.1786, train_acc=0.429, val_acc=0.428\n",
      "Epoch 65: train_loss=2.1788, train_acc=0.431, val_acc=0.433\n",
      "Epoch 66: train_loss=2.1815, train_acc=0.430, val_acc=0.431\n",
      "Epoch 67: train_loss=2.1770, train_acc=0.430, val_acc=0.432\n",
      "Epoch 68: train_loss=2.1793, train_acc=0.429, val_acc=0.424\n",
      "Epoch 69: train_loss=2.1757, train_acc=0.430, val_acc=0.432\n",
      "Epoch 70: train_loss=2.1805, train_acc=0.430, val_acc=0.429\n",
      "Epoch 71: train_loss=2.1793, train_acc=0.430, val_acc=0.422\n",
      "Epoch 72: train_loss=2.1765, train_acc=0.430, val_acc=0.429\n",
      "Epoch 73: train_loss=2.1767, train_acc=0.430, val_acc=0.434\n",
      "Epoch 74: train_loss=2.1797, train_acc=0.429, val_acc=0.432\n",
      "Epoch 75: train_loss=2.1741, train_acc=0.429, val_acc=0.429\n",
      "Epoch 76: train_loss=2.1752, train_acc=0.429, val_acc=0.430\n",
      "Epoch 77: train_loss=2.1765, train_acc=0.430, val_acc=0.420\n",
      "Epoch 78: train_loss=2.1786, train_acc=0.429, val_acc=0.431\n",
      "Epoch 79: train_loss=2.1784, train_acc=0.429, val_acc=0.421\n",
      "Epoch 80: train_loss=2.1852, train_acc=0.429, val_acc=0.429\n",
      "Epoch 81: train_loss=2.1801, train_acc=0.429, val_acc=0.429\n",
      "Epoch 82: train_loss=2.1774, train_acc=0.429, val_acc=0.432\n",
      "Epoch 83: train_loss=2.1776, train_acc=0.429, val_acc=0.425\n",
      "Epoch 84: train_loss=2.1766, train_acc=0.429, val_acc=0.434\n",
      "Epoch 85: train_loss=2.1778, train_acc=0.429, val_acc=0.429\n",
      "Epoch 86: train_loss=2.1775, train_acc=0.429, val_acc=0.430\n",
      "Epoch 87: train_loss=2.1837, train_acc=0.429, val_acc=0.423\n",
      "Epoch 88: train_loss=2.1822, train_acc=0.428, val_acc=0.424\n",
      "Epoch 89: train_loss=2.1889, train_acc=0.428, val_acc=0.429\n",
      "Epoch 90: train_loss=2.1957, train_acc=0.427, val_acc=0.420\n",
      "Epoch 91: train_loss=2.1946, train_acc=0.428, val_acc=0.426\n",
      "Epoch 92: train_loss=2.1901, train_acc=0.427, val_acc=0.428\n",
      "Epoch 93: train_loss=2.1929, train_acc=0.428, val_acc=0.429\n",
      "Epoch 94: train_loss=2.1944, train_acc=0.426, val_acc=0.427\n",
      "Epoch 95: train_loss=2.1865, train_acc=0.427, val_acc=0.426\n",
      "Epoch 96: train_loss=2.1848, train_acc=0.426, val_acc=0.422\n",
      "Epoch 97: train_loss=2.1878, train_acc=0.426, val_acc=0.424\n",
      "Epoch 98: train_loss=2.1885, train_acc=0.427, val_acc=0.430\n",
      "Epoch 99: train_loss=2.1828, train_acc=0.427, val_acc=0.429\n",
      "Epoch 100: train_loss=2.1946, train_acc=0.427, val_acc=0.424\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "n_channels = 10\n",
    "n_classes = 24\n",
    "model = CNN_LSTM(in_channels=n_channels, hidden_size=64, num_layers=2, n_classes=n_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss =criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "\n",
    "    train_loss = total_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    train_acc_history.append(train_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_total = 0\n",
    "    val_correct = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits = model(xb)\n",
    "            # loss =criterion(logits, yb)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_correct += (preds == yb).sum().item()\n",
    "            val_total += xb.size(0)\n",
    "\n",
    "    val_acc = val_correct/val_total\n",
    "\n",
    "    val_acc_history.append(val_acc)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e40add",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
